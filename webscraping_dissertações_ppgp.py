# -*- coding: utf-8 -*-
"""Webscraping - Dissertações PPGP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rIOAXG3aOQ1KdpeK2YmEht0irsERWFU7
"""

from types import NoneType
import requests
from bs4 import BeautifulSoup
import pandas as pd
from google.colab import files

# Fazer a requisição HTTP
url = 'http://repositorio.uninove.br/xmlui/handle/123456789/37/browse?order=ASC&rpp=20&sort_by=1&etal=-1&offset=0&type=title'

pags = [] # lista de páginas

for offset in range(0,161,20):
  urlpagpre = url.split("offset=")[0] + "offset="
  urlpag = urlpagpre + str(offset)
  pags.append(urlpag)

refs = [] # lista de referencia dos artigos

for p in pags:
  response = requests.get(p)

  # Analisar o conteúdo HTML da página
  soup = BeautifulSoup(response.text, 'html.parser')

  container = soup.find(class_='ds-artifact-list')

  # Encontrar todos os elementos 'a' dentro do container
  links = container.find_all('a')

  # Extrair os URLs dos links encontrados
  for link in links:
      refs.append(link['href'])

artifacts = [] # lista de url dos artigos

for ref in refs:
  urlartpre = url.split("/xmlui")[0]
  urlart = urlartpre + ref
  artifacts.append(urlart)

df = pd.DataFrame(columns=['titulo', 'autor', 'data', 'resumo'])

for artigo in artifacts:
    response1 = requests.get(artigo)

    soup1 = BeautifulSoup(response1.text, 'html.parser')

    titulo = soup1.find('h1').text
    autor = soup1.find(class_='simple-item-view-authors').text[1:-1]
    data = soup1.find_all(class_='simple-item-view-other')[1].text[7:-1]
    if soup1.find(class_='simple-item-view-description') == None:
      resumo = None
    else:
      resumo = soup1.find(class_='simple-item-view-description').text[11:-1]
    dados = pd.DataFrame({'titulo': [titulo],
                          'autor': [autor],
                          'data': [data],
                          'resumo': [resumo]})
    df = pd.concat([df, dados], ignore_index=True)

nome_arquivo = 'Dissertações PPGP.xlsx'
df.to_excel(nome_arquivo, index=False)
files.download(nome_arquivo)